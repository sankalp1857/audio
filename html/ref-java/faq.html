<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0016)http://localhost -->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.15"/>
<title>TrulyNatural SDK: Frequently Asked Questions</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
  /* @license-end */
</script>
<link rel="search" href="search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="Java Docs (7.4.0)"/>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 64px;">
  <td id="projectlogo"><img alt="Logo" src="sensory96.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">TrulyNatural SDK
   &#160;<span id="projectnumber">7.4.0</span>
   </div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
          <div class="left">
            <form id="FSearchBox" action="search.html" method="get">
              <img id="MSearchSelect" src="search/mag.png" alt=""/>
              <input type="text" id="MSearchField" name="query" value="Search" size="20" accesskey="S" 
                     onfocus="searchBox.OnSearchFieldFocus(true)" 
                     onblur="searchBox.OnSearchFieldFocus(false)"/>
            </form>
          </div><div class="right"></div>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<div id="language">
  <ul class="sm sm-dox" id="language-menu">
    <li id="label">API Language:</li>
    <li><a href="../ref-c/index.html">C</a></li>
    <li><a href="../ref-java/index.html" class="current">Java</a></li>
  </ul>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.15 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('faq.html','');});
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Frequently Asked Questions </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><ul>
<li><a class="el" href="faq.html#thread-safe">Is this SDK thread-safe?</a></li>
<li><a class="el" href="faq.html#debug-phrase-spotters">How do I debug a phrase spotter?</a></li>
<li><a class="el" href="faq.html#use-enrolled-triggers">How do I run Enrolled Fixed Trigger models?</a></li>
<li><a class="el" href="faq.html#run-two-spotters">Can I run two phrase spotters at the same time?</a></li>
<li><a class="el" href="faq.html#use-command-set">What is a Command Set?</a></li>
<li><a class="el" href="faq.html#trigger-to-search">Can I create a trigger-to-search model?</a></li>
<li><a class="el" href="faq.html#improve-user-experience">How do I improve the user experience in marginal conditions?</a></li>
<li><a class="el" href="faq.html#beta-model-compatibility">Can I use models from the beta releases?</a></li>
<li><a class="el" href="faq.html#improve-performance">How do I improve spotter performance?</a></li>
<li><a class="el" href="faq.html#use-lvcsr">How do I use Large Vocabulary Continuous Speech Recognition?</a></li>
</ul>
<h1><a class="anchor" id="thread-safe"></a>
Is this SDK thread-safe?</h1>
<p>Yes, as long as handles (such as <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1SnsrSession.html">SnsrSession</a> and <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1SnsrStream.html">SnsrStream</a>) are <b>not</b> shared between threads. The number of handles per thread are limited only by system resources.</p>
<p>If you need to share a <a href="https://www.sensory.com/natural-language-understanding/">TrulyNatural</a> SDK handle across threads, you <b>must</b> provide application-level mutual exclusion locking.</p>
<h1><a class="anchor" id="debug-phrase-spotters"></a>
How do I debug a phrase spotter?</h1>
<p>Create a new phrase spot model from the <a class="el" href="task.html#tpl-spot-debug">tpl-spot-debug</a> template, using the <a class="el" href="tools.html#snsr-edit">snsr-edit</a> tool.</p>
<p>Example: Create a debug spotter that listens for "hello blue genie"</p>
<div class="fragment"><div class="line">snsr-edit -t tpl-spot-debug-1.5.0.snsr\</div><div class="line">  -f 0 spot-hbg-enUS-1.4.0-m.snsr\</div><div class="line">  -o spot-hbg-debug.snsr</div></div><!-- fragment --><p>This sets slot <code>0</code> to <em>spot-hbg-enUS-1.4.0-m.snsr</em> and creates a new model file <em>spot-hbg-debug.snsr</em> </p>
<p>Test this model by running </p><div class="fragment"><div class="line">snsr-eval -t spot-hbg-debug.snsr -s debug-log-file=debug.log</div></div><!-- fragment --><p> and saying "hello blue genie"</p>
<p>This will produce a debug.log file. You could then use </p><div class="fragment"><div class="line">snsr-log-split debug.log</div></div><!-- fragment --><p> and get debug.txt and debug.wav and debug.snsr</p>
<p>The <code>debug-log-file</code> setting can also be set when creating the debug model, which makes it a drop-in replacement for the non-debug version:</p>
<div class="fragment"><div class="line">snsr-edit -t tpl-spot-debug-1.5.0.snsr\</div><div class="line">  -f 0 spot-hbg-enUS-1.4.0-m.snsr\</div><div class="line">  -s debug-log-file=debug.log\</div><div class="line">  -o spot-hbg-debug-0.snsr</div><div class="line"></div><div class="line">snsr-eval -t spot-hbg-debug-0.snsr</div></div><!-- fragment --><h1><a class="anchor" id="use-enrolled-triggers"></a>
How do I run Enrolled Fixed Trigger models?</h1>
<p>Enrolled Fixed Triggers use the same API, and follow the same enrollment recipe as User-Defined Triggers.</p>
<p>Replace the UDT model (<em>udt-universal-3.66.1.9.snsr</em>) in any of the samples with an EFT model such as <em>eft-hbg-enUS-23.0.0.9.snsr</em>.</p>
<h1><a class="anchor" id="run-two-spotters"></a>
Can I run two phrase spotters at the same time?</h1>
<p>Yes. Create a new phrase spot model from the <a class="el" href="task.html#tpl-spot-concurrent">tpl-spot-concurrent</a> template, using the <a class="el" href="tools.html#snsr-edit">snsr-edit</a> tool.</p>
<p>Example: Create a spotter that listens for "hello blue genie" and a music command set.</p>
<div class="fragment"><div class="line">snsr-edit -t tpl-spot-concurrent-1.4.0.snsr\</div><div class="line">  -f 0 spot-hbg-enUS-1.4.0-m.snsr\</div><div class="line">  -f 1 spot-music-enUS-1.2.0-m.snsr\</div><div class="line">  -o spot-hbg+music.snsr</div></div><!-- fragment --><p>This sets slot <code>0</code> to <em>spot-hbg-enUS-1.4.0-m.snsr</em> and slot <code>1</code> to <em>spot-music-enUS-1.2.0-m.snsr</em>, and creates a new model file <em>spot-hbg+music</em>.snsr</p>
<p>Test this model by running </p><div class="fragment"><div class="line">snsr-eval -t spot-hbg+music.snsr</div></div><!-- fragment --><p> and saying "hello blue genie", or one of the music set commands: "play music", "stop music", "next song", "previous song", or "pause
music".</p>
<h1><a class="anchor" id="use-command-set"></a>
What is a Command Set?</h1>
<p>Command sets are phrase spotters with more than one phrase. These are frequently tuned to have a limited listening window (<a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#ac8a82dd9408fbe5a6b40314e16424325">Snsr.LISTEN_WINDOW</a>).</p>
<p>Command set recognizers conform to the <a class="el" href="task.html#task-phrasespot">Phrase Spotter</a> specification, and can be used as a drop-in replacement for any phrase spotter. No code changes are required.</p>
<p>Most command sets are tuned for use after an always-listening keyword spotter. The <a class="el" href="task.html#tpl-spot-sequential">tpl-spot-sequential</a> template provides a convenient way to build such a model:</p>
<div class="fragment"><div class="line">snsr-edit -t tpl-spot-sequential-1.5.0.snsr\</div><div class="line">  -f 0 spot-hbg-enUS-1.4.0-m.snsr\</div><div class="line">  -f 1 spot-music-enUS-1.2.0-m.snsr\</div><div class="line">  -o spot-hbg-music.snsr</div></div><!-- fragment --><p>This sets slot <code>0</code> to <em>spot-hbg-enUS-1.4.0-m.snsr</em> and slot <code>1</code> to <em>spot-music-enUS-1.2.0-m.snsr</em>, and creates a new model file <em>spot-hbg-music.snsr</em> </p>
<p>Test this model by running </p><div class="fragment"><div class="line">snsr-eval -t spot-hbg-music.snsr</div></div><!-- fragment --><p> and saying "hello blue genie" <b>followed</b> by one of the music set commands: "play music", "stop music", "next song", "previous song", or "pause
music". If no music command is detected within five seconds after the trigger, <code>snsr-eval</code> will go back to listening for "hello blue
genie".</p>
<h1><a class="anchor" id="trigger-to-search"></a>
Can I create a trigger-to-search model?</h1>
<p>Yes. Create a new phrase spot model from the <a class="el" href="task.html#tpl-spot-vad">tpl-spot-vad</a> template, using the <a class="el" href="tools.html#snsr-edit">snsr-edit</a> tool.</p>
<div class="fragment"><div class="line">snsr-edit -t tpl-spot-vad-3.8.0.snsr\</div><div class="line">  -f 0 spot-hbg-enUS-1.4.0-m.snsr\</div><div class="line">  -o spot-hbg-vad.snsr</div></div><!-- fragment --><p>This creates a new model, <em>spot-hbg-vad.snsr</em> from the template by setting slot <code>0</code> to the "hello blue genie" spotter model.</p>
<p>Test this model by running </p><div class="fragment"><div class="line">snsr-eval -t spot-hbg-vad.snsr</div></div><!-- fragment --><p> and saying "Hello blue genie at what time does the sun rise in Chicago?". The audio following "Hello blue genie" is saved to <em>vad-audio.wav</em>.</p>
<p>See <a class="el" href="segmentSpottedAudio_8java-example.html">segmentSpottedAudio.java</a> for a Java code example.</p>
<h1><a class="anchor" id="improve-user-experience"></a>
How do I improve the user experience in marginal conditions?</h1>
<p>Use a spotter model with Smart Wakeword support. See <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a65809f44801589cc5fd5d795f47f49c5">Snsr.LOW_FR_OPERATING_POINT</a> and <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#acd8a20de817f9f03d69d4e0fb37f03bd">Snsr.DURATION_MS</a>.</p>
<h1><a class="anchor" id="beta-model-compatibility"></a>
Can I use models from the beta releases?</h1>
<p>Yes. This release is compatible with older models, but it requires a modification to the task requirement sanity checks. Use "~0.5.0 || 1.0.0" instead of "1.0.0", for example: </p><div class="fragment"><div class="line">session.require(Snsr.TASK_VERSION,  <span class="stringliteral">&quot;~0.5.0 || 1.0.0&quot;</span>);</div></div><!-- fragment --><p>The models included in the <a href="https://www.sensory.com/natural-language-understanding/">TrulyNatural</a> 6.0.0 release use <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#aa97f23479732b5b9a91fa5acaf2c8017">Snsr.TASK_VERSION</a> values of <code>1.0.0</code>. This makes these models incompatible with 5.0.0-beta releases.</p>
<h1><a class="anchor" id="improve-performance"></a>
How do I improve spotter performance?</h1>
<p><a class="el" href="contact.html">Contact Sensory</a> if interested in pursuing these customizations. There may be additional cost involved. Not all combinations may be possible depending on platform and trigger specification.</p>
<h2><a class="anchor" id="measure-real-time"></a>
How to measure real-time factor and MIPS</h2>
<p>To measure the real-time factor, time how long it takes to run the spotter over a long audio file. Then, real time factor = (run time in seconds) / (length of audio in seconds). To measure the MIPS on your device, use a profiler like perf when running the spotter over an audio file. Then, MIPS = (No. of instructions) / (length of audio in seconds * 1000000).</p>
<h2><a class="anchor" id="slower-than-rt"></a>
What if the spotter runs too slow, or consumes too many cycles?</h2>
<p>You could explore one of these options to see an improvement: Try <a class="el" href="faq.html#multi-threaded">multi-threaded</a>, <a class="el" href="faq.html#frame-stacked">frame-stacked</a>, or <a class="el" href="faq.html#little-big">little-big</a> spotters. You may also want to get a smaller spotter model, which uses less CPU (in proportion to its size) with a small reduction in FA and FR performance. Contact Sensory to see if these options are right for you.</p>
<h2><a class="anchor" id="low-ram"></a>
What if the spotter consumes too much memory?</h2>
<ol type="1">
<li>Contact Sensory for a smaller model.</li>
</ol>
<h2><a class="anchor" id="little-big"></a>
What is a little-big spotter?</h2>
<p>A little-big spotter does sequential recognition by first running a low-power spotter. When this spots, it re-processes the audio with a high-power state-of-the-art spotter. This reduces average CPU cycles (and hence power) required to run a spotter with a small increase in latency. This one combined model has the behavior of a high-power spotter.</p>
<h2><a class="anchor" id="frame-stacked"></a>
What is a frame-stacked spotter?</h2>
<p>Frame stacked spotters reduce the CPU load by 30-45%, in exchange for a small reduction in FA and FR performance.The resolution of time alignments is also reduced by a factor of two.</p>
<h2><a class="anchor" id="multi-threaded"></a>
What is a multi-threaded spotter?</h2>
<p>Multi-threaded spotters speed up execution on CPUs with more than one core.</p>
<h2><a class="anchor" id="Key"></a>
Factors to consider</h2>
<ol type="1">
<li>Are you willing to trade increased latency for fewer compute cycles?</li>
<li>Are you willing to distribute computation across multiple cores?</li>
<li>Yes to both 1 and 2?</li>
</ol>
<h1><a class="anchor" id="use-lvcsr"></a>
How do I use Large Vocabulary Continuous Speech Recognition?</h1>
<p>This <a href="https://www.sensory.com/natural-language-understanding/">TrulyNatural</a> release includes three different ways of running a speech-to-text recogizer: <a class="el" href="faq.html#lvcsr-no-vad">LVCSR without audio segmentation</a>, <a class="el" href="faq.html#lvcsr-vad">LVCSR with VAD-segmented audio</a>, and <a class="el" href="faq.html#lvcsr-spot-vad">LVCSR following a wake word</a>.</p>
<dl class="section note"><dt>Note</dt><dd>The <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#ab187127b4a74066f2de4a5698a8cc1a8">Snsr.RESULT_EVENT</a> callback only happens when a VAD endpoint is detected, or the end of the input stream is reached. For applications with live audio recognition, LVCSR recognizers should always be used with a <a class="el" href="task.html#task-vad">Voice Activity Detector</a>, such as <a class="el" href="task.html#tpl-spot-vad-lvcsr">tpl-spot-vad-lvcsr</a>, <a class="el" href="task.html#tpl-vad-lvcsr">tpl-vad-lvcsr</a>, or the experimental built-in lightweight VAD (configure <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a64327707ea513aadcf5d54b6616039fe">Snsr.LEADING_SILENCE</a> and <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a46ba153a3039b8867e6e228f85a2a82f">Snsr.TRAILING_SILENCE</a> for the recognizer).</dd></dl>
<h2><a class="anchor" id="lvcsr-no-vad"></a>
LVCSR without audio segmentation</h2>
<p>The <code>stt-enUS-automotive-medium-2.3.13.snsr</code> included in this distribution is a generic broad-domain US English speech-to-text recognizer with a special domain focus on automotive commands.</p>
<div class="fragment"><div class="line">$ bin/snsr-eval -t model/stt-enUS-automotive-medium-2.3.13.snsr \</div><div class="line">    data/enrollments/armadillo-1-3-c.wav</div><div class="line">P     40    200 im</div><div class="line">P     80    640 armadillo</div><div class="line">P    120   1120 Armadillo playing</div><div class="line">P    120   1520 Armadillo play marsa</div><div class="line">P    120   1880 Armadillo play more songs by</div><div class="line">P    120   2320 Armadillo play more songs by this art</div><div class="line">P    120   2600 Armadillo play more songs by this artist</div><div class="line">P    120   2640 Armadillo play more songs by this artist</div><div class="line">NLU intent: music_player (0.9849) = armadillo play more songs by this artist</div><div class="line">   120   2640 Armadillo play more songs by this artist.</div></div><!-- fragment --><p>Preliminary or partial results above are prefixed with <code>P</code>. Suppress these by setting the <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a0d7166f6f243e4356bb0b3e6471bbe4d">Snsr.PARTIAL_RESULT_INTERVAL</a> to <code>0</code>:</p>
<div class="fragment"><div class="line">$ bin/snsr-eval -t model/stt-enUS-automotive-medium-2.3.13.snsr \</div><div class="line">    -s partial-result-interval=0 \</div><div class="line">    data/enrollments/armadillo-1-3-c.wav</div><div class="line">NLU intent: music_player (0.9849) = armadillo play more songs by this artist</div><div class="line">   120   2640 Armadillo play more songs by this artist.</div></div><!-- fragment --><h2><a class="anchor" id="lvcsr-nlu"></a>
LVCSR with lightweight NLU parsing</h2>
<p>The <code>lvcsr-build-enUS-2.7.0.snsr</code> and stt-enUS-automotive-medium-2.3.13.snsr models support a lightweight natural language mark-up. This can significantly simplify application code that has to interpret recognition results. See <a class="el" href="build.html">Custom Recognition</a> for a description of the grammar syntax.</p>
<h3><a class="anchor" id="lvcsr-nlu-grammar"></a>
NLU with custom grammar recognizers</h3>
<div class="fragment"><div class="line">$ snsr-eval -t model/lvcsr-build-enUS-2.7.0.snsr \</div><div class="line">    -s partial-result-interval=0 \</div><div class="line">    -f grammar-stream data/grammars/enrollments-nlu-slot.txt \</div><div class="line">    data/enrollments/armadillo-1-4-c.wav</div><div class="line">NLU intent: avcontrol =  record a video</div><div class="line">NLU entity:   action = record</div><div class="line">NLU entity:   type = video</div><div class="line">   435   1995 armadillo record a video</div></div><!-- fragment --><h3><a class="anchor" id="lvcsr-nlu-bdlm"></a>
NLU with broad-domain recognizers</h3>
<p>In <a href="https://www.sensory.com/natural-language-understanding/">TrulyNatural</a> SDK 6.16.0 and later, NLU parsing is a separate processing step that occurs after the <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#ab187127b4a74066f2de4a5698a8cc1a8">Snsr.RESULT_EVENT</a>. NLU parsing includes a special <span class="tag">.</span> symbol that matches any input word. This allows crafting of more robust island parsers that can be used with free-form recognition results from a broad-domain model.</p>
<p>This small example detects a small set of microwave control commands using <a class="el" href="models.html#lvcsr-lib-enUS">lvcsr-lib-enUS-1.2.0.snsr</a>.</p>
<dl class="section note"><dt>Note</dt><dd>The stt-enUS-automotive-medium-2.3.13.snsr includes machine-learned NLU processing for automotive command tasks. If you use <code>nlu-grammar-stream</code> with this model the grammar-based NLU will override the machine-learned NLU parsing.</dd></dl>
<div class="fragment"><div class="line"># Microwave command NLU post-processor grammar</div><div class="line"># tiny-microwave.nlu</div><div class="line"></div><div class="line"># power level setting, &quot;fifty percent&quot;. don&#39;t capture optional &quot;power&quot;</div><div class="line">power = ~s.percent power?;</div><div class="line"></div><div class="line"># timer duration, &quot;two minutes and ten seconds&quot;</div><div class="line">duration = ~s.timer;</div><div class="line"></div><div class="line"># defrost command: the word &quot;defrost&quot; followed by</div><div class="line"># zero or more power or duration values, both captured</div><div class="line"># .* matches any input word sequence</div><div class="line">defrost = defrost ( .* ({power} | {duration}) .* )* ;</div><div class="line"></div><div class="line"># default action matches any input and discards it</div><div class="line">default = .:*;</div><div class="line"></div><div class="line"># set clock time: the word &quot;clock&quot; or &quot;time&quot; followed by</div><div class="line"># a time (&quot;seven twenty nine pm&quot;).</div><div class="line"># ignore spurious words before and after the time specification</div><div class="line">clock = (clock | time) .* {time ~s.time} .*;</div><div class="line"></div><div class="line"># list of all the actions we&#39;ve defined, captured</div><div class="line">action = {defrost} | {clock} | {default};</div><div class="line"></div><div class="line"># match any one of the actions, ignoring unknown words before</div><div class="line"># and after</div><div class="line">nlu = &lt;s&gt; .* $action .* &lt;/s&gt;;</div></div><!-- fragment --><p>Build and run a recognizer with live input. </p><div class="fragment"><div class="line">$ snsr-eval -v -t model/tpl-vad-lvcsr-3.14.0.snsr \</div><div class="line">    -f 0 model/stt-enUS-automotive-medium-2.3.13.snsr \</div><div class="line">    -t model/lvcsr-lib-enUS-1.2.0.snsr \</div><div class="line">    -f nlu-grammar-stream tiny-microwave.nlu \</div><div class="line">    -s partial-result-interval=0</div><div class="line">Using live audio from default capture device. ^C to stop.</div><div class="line"></div><div class="line"># &quot;hi there, please defrost my soup for 15 minutes at 30% power&quot;</div><div class="line">   495   6975 [^end] VAD speech region.</div><div class="line">NLU intent: defrost (0.0000) = defrost my soup for 15 minutes at thirty percent power</div><div class="line">NLU entity:   duration (0.0000) = 15 minutes</div><div class="line">NLU entity:   power (0.0000) = thirty percent power</div><div class="line">   850   6690 (0.4248) Hi there, please defrost my soup for fifteen minutes at thirty percent power.</div><div class="line"></div><div class="line"># &quot;could you set the clock to 3:43 pm?&quot;</div><div class="line">NLU intent: clock (0.0000) = clock to 15:43</div><div class="line">NLU entity:   time (0.0000) = 15:43</div><div class="line">   430   3190 (0.1353) Could you set the clock to three? Forty three P? M.</div></div><!-- fragment --><h3><a class="anchor" id="lvcsr-nlu-nbest"></a>
Dealing with NLU parse ambiguity</h3>
<p>It is possible to get more than one valid parse result if the NLU grammar introduces ambiguity. The NLU processor scores these alternates and returns the best hypotheses in order, up to <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a1e5f48a68207599b223b0c5d5272270b">Snsr.NLU_RES_MAX</a>. During the <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a5f39483b71c15d813a6fda5c0f8948a2">Snsr.NLU_SLOT_EVENT</a> callback, <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#afae6bcf7a33b26e30cabfe128dd38fc7">Snsr.RES_NLU_COUNT</a> reports the number of alternates available, and <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a2b591029381243f5fd253b891fc4aa05">Snsr.RES_NLU_INDEX</a> the current alternate.</p>
<p><a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a1e5f48a68207599b223b0c5d5272270b">Snsr.NLU_RES_MAX</a> defaults to <code>1</code> for best compatibility with earlier releases.</p>
<p>This example uses two NLU grammars: <em>system.nlu</em> for basic functionality provided by a product, and <em>app.nlu</em> to extend NLU processing for a plug-in application. If the application duplicates some of the system NLU actions we need these reported for the system to take appropriate action.</p>
<div class="fragment"><div class="line"># system.nlu</div><div class="line">volume = volume: {volume-level ~s.percent};</div><div class="line">preset = preset: number:? ~s.number-integer-0-9;</div><div class="line">system = {volume} | {preset};</div><div class="line"># :/-0.1 adds a small weight bias towards the ~app class, so</div><div class="line"># ~app will outscore $system for identical matches</div><div class="line">plugin = :/-0.1 ~app;</div><div class="line">action = {system} | {plugin};</div><div class="line">nlu = &lt;s&gt; $action &lt;/s&gt;;</div></div><!-- fragment --><div class="fragment"><div class="line"># app.nlu</div><div class="line">media-control = ~s.control.media;</div><div class="line">preset = preset: ( one | two | three | four | five );</div><div class="line">nlu = {media-control} | {preset};</div></div><!-- fragment --><p>Build and run a recognizer with live input. Set the string value for <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#a1e5f48a68207599b223b0c5d5272270b">Snsr.NLU_RES_MAX</a> to allow up to ten alternate matches. This example uses a machine-learned VAD.</p>
<div class="fragment"><div class="line">snsr-eval -vvt model/tpl-vad-lvcsr-3.14.0.snsr \</div><div class="line">    -f 0 lvcsr-broad-enUS-2.3.0.snsr \</div><div class="line">    -t model/lvcsr-lib-enUS-1.2.0.snsr \</div><div class="line">    -f nlu-grammar-stream system.nlu \</div><div class="line">    -f nlu-grammar-stream.app app.nlu \</div><div class="line">    -s partial-result-interval=0 \</div><div class="line">    -s nlu-match-max=10</div><div class="line"></div><div class="line"><span class="preprocessor"># &quot;volume 50%&quot;</span></div><div class="line"><span class="preprocessor"># in system</span></div><div class="line">  3540 [^begin]</div><div class="line">  3015   5805 [^end] VAD speech region.</div><div class="line">NLU  1/1 nlu-slot-value.system (0.0000) = { volume { volume-level fifty percent } }</div><div class="line">NLU  1/1 nlu-slot-value.system.volume (0.0000) = { volume-level fifty percent }</div><div class="line">NLU  1/1 nlu-slot-value.system.volume.volume-level (0.0000) = fifty percent</div><div class="line">phrase:</div><div class="line">  3375   4770 (0.0000 sv) fifty percent</div><div class="line">words:</div><div class="line">  3375   3930 (0.0000 sv) volume</div><div class="line">  3930   4305 (0.0000 sv) fifty</div><div class="line">  4305   4770 (0.0000 sv) percent</div><div class="line"></div><div class="line"><span class="preprocessor"># &quot;fast forward&quot;</span></div><div class="line"><span class="preprocessor"># in plugin</span></div><div class="line"> 50820 [^begin]</div><div class="line"> 50295  51840 [^end] VAD speech region.</div><div class="line">NLU  1/1 nlu-slot-value.plugin (0.0000) = { media-control fast forward }</div><div class="line">NLU  1/1 nlu-slot-value.plugin.media-control (0.0000) = fast forward</div><div class="line">phrase:</div><div class="line"> 50565  51540 (0.0000 sv) fast forward</div><div class="line">words:</div><div class="line"> 50565  50925 (0.0000 sv) fast</div><div class="line"> 50925  51540 (0.0000 sv) forward</div><div class="line"></div><div class="line"># <span class="stringliteral">&quot;preset 5&quot;</span></div><div class="line"># in both system and plugin, plugin listed first due to weight bias</div><div class="line">  1575 [^begin]</div><div class="line">  1050   3330 [^end] VAD speech region.</div><div class="line">NLU  1/2 nlu-slot-value.plugin (0.0000) = { preset five }</div><div class="line">NLU  1/2 nlu-slot-value.plugin.preset (0.0000) = five</div><div class="line">NLU  2/2 nlu-slot-value.system (0.0000) = { preset five }</div><div class="line">NLU  2/2 nlu-slot-value.system.preset (0.0000) = five</div><div class="line">phrase:</div><div class="line">  1320   2985 (0.0000 sv) five</div><div class="line">words:</div><div class="line">  1320   1965 (0.0000 sv) preset</div><div class="line">  2295   2985 (0.0000 sv) five</div><div class="line"></div><div class="line"># <span class="stringliteral">&quot;preset 0&quot;</span></div><div class="line"># in system</div></div><!-- fragment --><h3><a class="anchor" id="lvcsr-nlu-action"></a>
How do I take action on an NLU result?</h3>
<p>You can think of an intent as specifying which function or method you should call to perform an action. Entities identify parts of the utterance that include additional detail. For example, a <code>call_contact</code> intent might have a <code>contact_name</code> entity that specifies who to call.</p>
<ul>
<li>Register a handler for <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#af563ab3901db5eeab35837a2587bf253">Snsr.NLU_INTENT_EVENT</a></li>
<li>In this handler,<ul>
<li>Retrieve <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#acaab83cdc1ebf7f16bf914dd1e73ff07">Snsr.RES_NLU_INTENT_NAME</a> as a string.</li>
<li>Map this intent name to an action. Do this by comparing the intent name to all valid intent names for which you want to perform an action.</li>
<li>If the matched action requires additional data, retrieve the expected <a class="el" href="classcom_1_1sensory_1_1speech_1_1snsr_1_1Snsr.html#afbd509ff3b7ced88a21667173cc54395">Snsr.RES_NLU_ENTITY_VALUE</a> by name.</li>
<li>Call a function (specified by the intent value) with zero or more arguments specified by the entity values.</li>
<li>Return from the intent event handler with <a class="el" href="enumcom_1_1sensory_1_1speech_1_1snsr_1_1SnsrRC.html#a509f04652b5e1b2d2b9e6bc121a87e50">SnsrRC.OK</a>.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="lvcsr-vad"></a>
LVCSR with VAD-segmented audio</h2>
<p>Large vocabulary recognizers perform better when used with a Voice Activity Detector that removes extraneous leading and trailing silence.</p>
<p>Create such a VAD-lvcsr model using the <a class="el" href="task.html#tpl-vad-lvcsr">tpl-vad-lvcsr</a> template: </p><div class="fragment"><div class="line">$ bin/snsr-edit -t model/tpl-vad-lvcsr-3.14.0.snsr \</div><div class="line">    -f 0 model/stt-enUS-automotive-medium-2.3.13.snsr \</div><div class="line">    -o vad-stt-enUS-automotive-medium-2.3.13.snsr</div></div><!-- fragment --><p>Evaluate using <a class="el" href="tools.html#snsr-eval">snsr-eval</a>. </p><div class="fragment"><div class="line">$ bin/snsr-eval -t vad-stt-enUS-automotive-medium-2.3.13.snsr \</div><div class="line">    data/enrollments/armadillo-1-0-c.wav</div><div class="line">P    230    830 armadilla</div><div class="line">P    270   1150 Armadillo, eight</div><div class="line">P    310   1630 Armadillo, eighteen percent</div><div class="line">P    310   1910 Armadillo. Eighteen percent of s</div><div class="line">P    310   2430 Armadillo, eighteen percent of six hundred</div><div class="line">P    310   2790 Armadillo, eighteen percent of six hundred and forty</div><div class="line">P    310   3150 Armadillo, eighteen percent of six hundred forty three</div><div class="line">NLU intent: no_command (0.9765) = armadillo eighteen percent of 643</div><div class="line">NLU entity:   number (0.9564) = 643</div><div class="line">   310   3190 Armadillo, eighteen percent of six hundred forty three.</div></div><!-- fragment --><h2><a class="anchor" id="lvcsr-spot-vad"></a>
LVCSR following a wake word</h2>
<p>The <a class="el" href="task.html#tpl-spot-vad-lvcsr">tpl-spot-vad-lvcsr</a> template provides a way to start a large-vocabulary recognizer with a spotted wake word. In this example we'll enroll a wake-word, then use the enrolled spotter with the broad-domain recognizer.</p>
<p>Create an enrolled spotter for "jackalope": </p><div class="fragment"><div class="line">$ spot-enroll -v -t model/udt-universal-3.66.1.9.snsr +jackalope-1 \</div><div class="line">  data/enrollments/jackalope-1-0.wav \</div><div class="line">  data/enrollments/jackalope-1-1.wav \</div><div class="line">  data/enrollments/jackalope-1-4.wav \</div><div class="line">  data/enrollments/jackalope-1-3.wav</div><div class="line">Adapting: 100% complete.</div><div class="line">Enrolled model saved to &quot;enrolled-sv.snsr&quot;</div></div><!-- fragment --><p>Combine the enrolled spotter and the broad-domain recognizer using the <a class="el" href="task.html#tpl-spot-vad-lvcsr">tpl-spot-vad-lvcsr</a> template: </p><div class="fragment"><div class="line">$ snsr-edit -v -t model/tpl-spot-vad-lvcsr-3.15.0.snsr \</div><div class="line">  -f 0 enrolled-sv.snsr \</div><div class="line">  -f 1 model/stt-enUS-automotive-medium-2.3.13.snsr \</div><div class="line">  -s include-leading-silence=1 \</div><div class="line">  -o jackalope-stt-enUS-automotive-medium-2.3.13.snsr</div><div class="line">Saved edited model to &quot;jackalope-stt-enUS-automotive-medium-2.3.13.snsr&quot;.</div></div><!-- fragment --><p>Evaluate using <a class="el" href="tools.html#snsr-eval">snsr-eval</a>. Note that the wake word is not included in the LVCSR transcription. </p><div class="fragment"><div class="line">$ snsr-eval -t jackalope-stt-enUS-automotive-medium-2.3.13.snsr \</div><div class="line">    data/enrollments/jackalope-1-2-c.wav</div><div class="line">P   1050   1530 directions</div><div class="line">P   1050   1930 Directions to sus</div><div class="line">P   1050   2370 Directions to Susan&#39;s house</div><div class="line">NLU intent: navigation (0.9973) = directions to susan&#39;s house</div><div class="line">NLU entity:   navigation_location (0.9811) = susan&#39;s house</div><div class="line">  1050   2530 Directions to Susan&#39;s house.</div></div><!-- fragment --> </div></div><!-- PageDoc -->
</div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.11-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
    Copyright &copy;2002-2025
    <a href="http://www.sensory.com/">Sensory Inc.</a>
    (Mon Feb 17 2025)
    <span class="confidential">
      <a href="license.html">Sensory Confidential</a>
    </span>
    </li>
  </ul>
</div>
</body>
</html>
